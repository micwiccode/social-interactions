{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kgyaMM9Xx41",
    "outputId": "f9fb31ee-46f0-4504-a2e4-50ebdf4efa76"
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xB9-iQ4jfjgI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from statistics import mean\n",
    "import pickle\n",
    "import random\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from typing import List, Dict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tqd5a-7QYZZO"
   },
   "outputs": [],
   "source": [
    "useSyntheticData = True\n",
    "\n",
    "n_types = 5\n",
    "n_epoch = 15\n",
    "seq_len = 750\n",
    "batch_size = 10\n",
    "hidden_size = 64\n",
    "# RELU | SOFTPLUS | SOFTPLUS_SCALE\n",
    "transform_fun = 'SOFTPLUS_SCALE'\n",
    "# W2V = 2, GCN = 1\n",
    "number_of_embedding_features = 2\n",
    "number_of_spatio_features = 2\n",
    "# p = 1 for GCN and W2V\n",
    "windows_p = 6\n",
    "\n",
    "use_data_from_files = True \n",
    "\n",
    "# NN | GCN | TGCN | W2V | TW2V\n",
    "model = 'TW2V'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "id": "MFuEwU0Eflaz",
    "outputId": "11f43fb8-6ec2-46cf-9ce1-59655acebfd5"
   },
   "outputs": [],
   "source": [
    "if(useSyntheticData):\n",
    "  events_df = pd.read_csv('./social-interactions/events_syn.csv')\n",
    "else:\n",
    "  events_df = pd.read_csv('./social-interactions/events.csv')  \n",
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbFY5x0vW0JN"
   },
   "outputs": [],
   "source": [
    "events_df = events_df[['epochtime', 'egoid', 'alterid', 'eventtype']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYXlXRlzfrhO"
   },
   "outputs": [],
   "source": [
    "events_df = events_df.sort_values(by=['epochtime'], ascending=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lY9OvHYaO6wx"
   },
   "outputs": [],
   "source": [
    "if('_id' not in events_df.columns):\n",
    "  events_df['_id'] = events_df.index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eventTypeSynMap(e_type):\n",
    "  return {\n",
    "        'A': 0,\n",
    "        'B': 1,\n",
    "        'C': 2,\n",
    "        'D': 3,\n",
    "        'E': 4\n",
    "    }.get(e_type, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexEventTypeSynMap(e_type):\n",
    "  return {\n",
    "        0: 'A',\n",
    "        1: 'B',\n",
    "        2: 'C',\n",
    "        3: 'D',\n",
    "        4: 'E'\n",
    "    }.get(e_type, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80MB-3Safuno"
   },
   "outputs": [],
   "source": [
    "def eventTypeMap(e_type):\n",
    "  if(useSyntheticData): \n",
    "    return eventTypeSynMap(e_type)\n",
    "  return {\n",
    "        'Call': 0,\n",
    "        'MMS': 1,\n",
    "        'SMS': 2,\n",
    "        'WhatsApp': 3\n",
    "    }.get(e_type, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmjkku1Ovi3o"
   },
   "outputs": [],
   "source": [
    "def indexEventTypeMap(index):\n",
    "  if(useSyntheticData): \n",
    "    return indexEventTypeSynMap(index)\n",
    "  return {\n",
    "        0: 'Call',\n",
    "        1: 'MMS',\n",
    "        2: 'SMS',\n",
    "        3: 'WhatsApp'\n",
    "    }.get(index, 'Call')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MOYXbhHZRR5"
   },
   "outputs": [],
   "source": [
    "def getCuda():\n",
    "  gpu_avail = torch.cuda.is_available()\n",
    "  print(f\"Is the GPU available? {gpu_avail}\")\n",
    "\n",
    "\n",
    "  if(gpu_avail):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    print(\"Device\", device)\n",
    "\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "  cuda = torch.device('cuda')\n",
    "  return cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfE6Luk8ty6y"
   },
   "outputs": [],
   "source": [
    "def toTensor(events, events_ids, times, times_max, embedding_features, cuda):\n",
    "  timeScale = 1.0\n",
    "  if cuda:\n",
    "    t_events = torch.tensor(events, device=cuda).float()\n",
    "    t_events_ids = torch.tensor(events_ids, device=cuda).long()\n",
    "    t_times = torch.tensor(times/timeScale, device=cuda).float()\n",
    "    t_times_max = torch.tensor(times_max/timeScale, device=cuda).float()\n",
    "    t_embedding_features = torch.tensor(embedding_features, device=cuda).float()\n",
    "  else: \n",
    "    t_events = torch.tensor(events).float()\n",
    "    t_events_ids = torch.tensor(events_ids).long()\n",
    "    t_times = torch.tensor(times/timeScale).float()\n",
    "    t_times_max = torch.tensor(times_max/timeScale).float()\n",
    "    t_embedding_features = torch.tensor(embedding_features).float()\n",
    "\n",
    "  return t_events, t_events_ids, t_times, t_times_max, t_embedding_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBgTwP7tXGKi"
   },
   "outputs": [],
   "source": [
    "def getPathDir():\n",
    "  return {\n",
    "        'NN': 'nn',\n",
    "        'GCN': 'gcn',\n",
    "        'TGCN': 'tgcn',\n",
    "        'W2V': 'w2v',\n",
    "        'TW2V': 'tw2v'\n",
    "    }.get(model, 'nn')\n",
    "\n",
    "database = 'syn' if useSyntheticData else 'nethealth'\n",
    "\n",
    "def getPath(e_type):\n",
    "  dir = getPathDir()\n",
    "  return './social-interactions/{}/{}_{}_{}_{}.pkl'.format(dir, e_type, seq_len, windows_p, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVTueIhptTGa"
   },
   "outputs": [],
   "source": [
    "def readData():\n",
    "  with open(getPath('events'), 'rb') as fid:\n",
    "     events = pickle.load(fid)\n",
    "  with open(getPath('events_ids'), 'rb') as fid:\n",
    "     events_ids = pickle.load(fid)\n",
    "  with open(getPath('times'), 'rb') as fid:\n",
    "     times = pickle.load(fid)\n",
    "  with open(getPath('times_max'), 'rb') as fid:\n",
    "     times_max = pickle.load(fid)\n",
    "  with open(getPath('embedded'), 'rb') as fid:\n",
    "     all_embedded_events = pickle.load(fid)\n",
    "\n",
    "  return events, events_ids, times, times_max, all_embedded_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-Pw1-x-tUYD"
   },
   "outputs": [],
   "source": [
    "def saveData(events, events_ids, times, times_max, all_embedded_events):\n",
    "  with open(getPath('events'), 'wb') as fid:\n",
    "     pickle.dump(events, fid)\n",
    "  with open(getPath('events_ids'), 'wb') as fid:\n",
    "     pickle.dump(events_ids, fid)\n",
    "  with open(getPath('times'), 'wb') as fid:\n",
    "     pickle.dump(times, fid)\n",
    "  with open(getPath('times_max'), 'wb') as fid:\n",
    "     pickle.dump(times_max, fid)\n",
    "  with open(getPath('embedded'), 'wb') as fid:\n",
    "     pickle.dump(all_embedded_events, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYb00yz2s59J"
   },
   "outputs": [],
   "source": [
    "def encodeEvents(events):\n",
    "  num_of_seq, num_of_ev_per_seq = events.shape\n",
    "  events_one_hot = np.zeros((num_of_seq, num_of_ev_per_seq, n_types))\n",
    "  \n",
    "  for seq in range(num_of_seq):\n",
    "      for step in range(num_of_ev_per_seq):\n",
    "          ev = events[seq, step]\n",
    "          events_one_hot[seq, step, ev] = 1.0\n",
    "\n",
    "  return events_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6iod2lgQxKy"
   },
   "outputs": [],
   "source": [
    "def normalize(embedding):\n",
    "  if(np.min(embedding) == np.max(embedding)):\n",
    "    embedding_result = np.zeros_like(embedding).tolist()\n",
    "  else:\n",
    "    embedding_result = (embedding - np.min(embedding)) / (np.max(embedding) - np.min(embedding)).tolist()\n",
    "\n",
    "  return embedding_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vo383R2lC2FO"
   },
   "outputs": [],
   "source": [
    "# Calc skipgram embeddings features at vector_size dimentions real space \n",
    "# neighbourhoods - sampled neighbourhoods\n",
    "# Word2Vec params:\n",
    "# vector_size: number of dim at skipgram embeddings\n",
    "# window size for skipgram model\n",
    "# min_event_count: minimum number occurences of event (events with less occurrences are skipped)\n",
    "# epochs: number for epochs for skipgram training\n",
    "\n",
    "def getSkipGramEmbeddings(neighbourhoods, vector_size ,window_size = 3, min_event_count = 1, epochs = 1):\n",
    "    skip_gram_model = Word2Vec(sentences=neighbourhoods, \n",
    "                    vector_size=vector_size, \n",
    "                    window=window_size, \n",
    "                    min_count=min_event_count, \n",
    "                    epochs=epochs,\n",
    "                    sg=1)\n",
    "    \n",
    "    # This module implements word vectors, and more generally sets of vectors keyed by lookup tokens/ints\n",
    "    features = skip_gram_model.wv.get_normed_vectors()\n",
    "    keys = skip_gram_model.wv.index_to_key\n",
    "\n",
    "    embeddings = {keys[idx]: features[idx] for idx in range(len(keys))}\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fb7z-A2_Qucf"
   },
   "outputs": [],
   "source": [
    "def addMissingNodes(embeddings, events_ids, n_space_features):\n",
    "  embeddings_keys = list(map(int, embeddings.keys())) \n",
    "  result_embeddings = np.zeros((len(events_ids), n_space_features))\n",
    "\n",
    "  for idx, events_id in enumerate(events_ids):\n",
    "    if(events_id in embeddings_keys):\n",
    "      result_embeddings[idx] = embeddings[events_id]\n",
    "    else:\n",
    "      for i in range(n_space_features):\n",
    "        result_embeddings[idx][i] = 0\n",
    "\n",
    "  return result_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itVPe-AAx6Qx"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, List\n",
    "\n",
    "def calculateEventGraphWeights(df, delta_t: int = 1):\n",
    "    number_of_interactions = len(df)\n",
    "    path_dict = {}\n",
    "    co_dict = {}\n",
    "    event_between_nodes = {}\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        source_node_1, target_node_1, timestamp_1 = row['source'], row['target'], row['timestamp']\n",
    "        event_between_nodes[i] = (source_node_1, target_node_1)\n",
    "\n",
    "        # node_sharing - dataframe przyszłych zdarzeń gdzie nadawca lub odbiorca są podmiotami tych zdarzeń \n",
    "        node_sharing = df[((df['source'].isin([source_node_1, target_node_1])) | (df['target'].isin([source_node_1, target_node_1]))) & (df['timestamp'] > timestamp_1)]\n",
    "        # w_path = 1 / (1 + (tablica wartości bezwzględnych z różnic czasowych między obecnym, a przyszłymi zdarzeniami)) \n",
    "        w_path = 1 / (1 + abs(node_sharing['timestamp'] - timestamp_1))\n",
    "\n",
    "        for j, other_row in node_sharing.iterrows():\n",
    "            source_node_2, target_node_2, timestamp_2 = other_row['source'], other_row['target'], other_row['timestamp']\n",
    "            path_dict[(i, j)] = w_path[j]\n",
    "            # jeśli różnica czasu między kolejnymi zdarzeniami tych osób jest mniejsza od parametru delta_t\n",
    "            # oblicza co_dict, która mówi o liczbie 'delta_t-adjacent' zdarzeń w krawędziach pomiędzy (event_i_source, event_i_target) and (event_j_source, event_j_target)\n",
    "            if timestamp_2 - timestamp_1 <= delta_t:\n",
    "                co_dict[(source_node_1, target_node_1, source_node_2, target_node_2)] = co_dict.get((source_node_1, target_node_1, source_node_2, target_node_2), 0) + 1\n",
    "                co_dict[(source_node_2, target_node_2, source_node_1, target_node_1)] = co_dict[(source_node_1, target_node_1, source_node_2, target_node_2)]\n",
    "\n",
    "    return path_dict, co_dict, event_between_nodes\n",
    "\n",
    "\n",
    "def createEdgesList(path_dict: Dict[Tuple[int, int], float]) -> np.array:\n",
    "    edges = np.empty((2, len(path_dict.keys())))\n",
    "    for i, (event_a, event_b) in enumerate(path_dict.keys()):\n",
    "        edges[0, i] = event_a\n",
    "        edges[1, i] = event_b\n",
    "    return edges\n",
    "\n",
    "\n",
    "def sampleNeighbourhoods(path_dict, co_dict, event_between_nodes, alpha = 0.5, nb = 10, s = 5):\n",
    "\n",
    "    def _sampleNodeNeighbourhoods(node, neighbors, probs):\n",
    "        if len(neighbors) < s:\n",
    "            return [[node] + list(np.random.choice(neighbors, size=s, replace=True, p=probs)) for _ in range(nb)]\n",
    "        else:\n",
    "            return [[node] + list(np.random.choice(neighbors, size=s, replace=True, p=probs)) for _ in range(nb)]\n",
    "\n",
    "    neighbourhoods = []\n",
    "    edges = createEdgesList(path_dict)\n",
    "    nodes = np.unique(edges)\n",
    "\n",
    "    for node in nodes:\n",
    "        source_node, target_node = event_between_nodes[node]\n",
    "\n",
    "        predecessors = edges[0, edges[1, :] == node]\n",
    "        successors = edges[1, edges[0, :] == node]\n",
    "        neighbors = np.concatenate((predecessors, successors), axis=None)\n",
    "\n",
    "        if neighbors.size == 0:\n",
    "            continue\n",
    "        F_path_weigth_normalize = sum([path_dict[(pred, node)] for pred in predecessors]) + \\\n",
    "                             sum([path_dict[(node, succ)] for succ in successors])\n",
    "        F_co_weigth_normalize = sum([co_dict.get((*event_between_nodes[pred], source_node, target_node), 0) for pred in predecessors]) + \\\n",
    "                           sum([co_dict.get((source_node, target_node, *event_between_nodes[succ]), 0) for succ in successors])\n",
    "\n",
    "        if F_path_weigth_normalize == 0:\n",
    "            probabilities = [(1 - alpha) * co_dict.get((*event_between_nodes[pred], source_node, target_node), 0) / F_co_weigth_normalize\n",
    "                             for pred in predecessors] + \\\n",
    "                            [(1 - alpha) * co_dict.get((source_node, target_node, *event_between_nodes[succ]), 0) / F_co_weigth_normalize\n",
    "                             for succ in successors]\n",
    "        elif F_co_weigth_normalize == 0:\n",
    "            probabilities = [alpha * path_dict[(pred, node)] / F_path_weigth_normalize for pred in predecessors] + \\\n",
    "                            [alpha * path_dict[(node, succ)] / F_path_weigth_normalize for succ in successors]\n",
    "        else:\n",
    "            probabilities = [alpha * path_dict[(pred, node)] / F_path_weigth_normalize +\n",
    "                             (1 - alpha) * co_dict.get((*event_between_nodes[pred], source_node, target_node), 0) / F_co_weigth_normalize\n",
    "                             for pred in predecessors] + \\\n",
    "                            [alpha * path_dict[(node, succ)] / F_path_weigth_normalize +\n",
    "                             (1 - alpha) * co_dict.get((source_node, target_node, *event_between_nodes[succ]), 0) / F_co_weigth_normalize\n",
    "                             for succ in successors]\n",
    "        probabilities = probabilities / sum(probabilities)\n",
    "\n",
    "        node_nbrhds = _sampleNodeNeighbourhoods(node, neighbors, probabilities)\n",
    "        neighbourhoods.extend(node_nbrhds)\n",
    "\n",
    "    return np.array(neighbourhoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVYe6w1a1egS"
   },
   "outputs": [],
   "source": [
    "def getEncodedEventsWEG2VEC(temporal_graphs, seq_len, number_of_embedding_features):\n",
    "  embeddings_matrix = np.zeros((seq_len, number_of_embedding_features))\n",
    "  event_idx = 0\n",
    "\n",
    "  for idx, _graph in enumerate(temporal_graphs):\n",
    "    _graph = _graph[['_id', 'source', 'target', 'time']]\n",
    "    _graph.columns = ['_id', 'source', 'target', 'timestamp']\n",
    "    events_ids = _graph['_id'].tolist()\n",
    "\n",
    "    path_dict, co_dict, event_between_nodes = calculateEventGraphWeights(_graph)\n",
    "    neighbourhoods = sampleNeighbourhoods(path_dict, co_dict, event_between_nodes, alpha = 0.5, nb = 10, s = number_of_embedding_features)\n",
    "\n",
    "    embeddings = getSkipGramEmbeddings(neighbourhoods.tolist(), number_of_embedding_features)\n",
    "    embeddings = addMissingNodes(embeddings, events_ids, number_of_embedding_features)\n",
    "\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "      embeddings_matrix[event_idx][0] = embedding[0]\n",
    "      embeddings_matrix[event_idx][1] = embedding[1]\n",
    "      event_idx += 1\n",
    "\n",
    "\n",
    "  return embeddings_matrix.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AiWk-Xl_s8Hz"
   },
   "outputs": [],
   "source": [
    "def generateData(df, seq_len, windows_p, number_of_embedding_features):\n",
    "  number_of_seqs = math.floor(df.shape[0] / seq_len)\n",
    "  events = np.zeros([number_of_seqs, seq_len], dtype=int)\n",
    "  embedded_events = []\n",
    "  times = np.zeros([number_of_seqs, seq_len+1], dtype=int)\n",
    "  times_max = np.zeros(number_of_seqs, dtype=int)\n",
    "\n",
    "  targets = []\n",
    "  sources = []\n",
    "  types = []\n",
    "  _ids = []\n",
    "\n",
    "  curr_seq = 0\n",
    "  curr_step = 0\n",
    "  first_seq_time = 0\n",
    "\n",
    "  for index, row in tqdm(df.iterrows(), position=0, leave=True):\n",
    "    if(curr_seq >= number_of_seqs):\n",
    "      break\n",
    "\n",
    "    curr_step = index % seq_len\n",
    "    if curr_step == 0:\n",
    "       first_seq_time = row['epochtime']\n",
    "\n",
    "    event_type = eventTypeMap(row['eventtype'])\n",
    "    events[curr_seq][curr_step] = event_type\n",
    "    times[curr_seq][curr_step] = (row['epochtime'] - first_seq_time) / 1000\n",
    "\n",
    "    targets.append(row['alterid'])\n",
    "    sources.append(row['egoid'])\n",
    "    _ids.append(row['_id'])\n",
    "    types.append(event_type)\n",
    "\n",
    "    if curr_step == seq_len - 1:\n",
    "      times_max[curr_seq] = np.max(times[curr_seq])\n",
    "      times[curr_seq][curr_step+1] = (df.iloc[[index + 1]]['epochtime'] - first_seq_time) / 1000\n",
    "        \n",
    "      graph_times = times[curr_seq][:-1]\n",
    "      graphs_df = pd.DataFrame(data={'_id':_ids, 'time': graph_times, 'source':sources, 'target': targets, 'type': types})\n",
    "      min_time = min(graph_times)\n",
    "      max_time = max(graph_times)\n",
    "      window_duration = (max_time - min_time) / windows_p\n",
    "      temporal_graphs = []\n",
    "                     \n",
    "      for window in range(windows_p):\n",
    "            begin = min_time + (window_duration * window)\n",
    "            end = min_time + (window_duration * (window+1))\n",
    "            temporal_graphs.append(graphs_df.loc[(graphs_df['time'] >= begin) & (graphs_df['time'] < end)])\n",
    "                     \n",
    "      encoded_events = getEncodedEventsWEG2VEC(temporal_graphs, seq_len, number_of_embedding_features)\n",
    "      embedded_events.append(encoded_events)\n",
    "\n",
    "      targets = []\n",
    "      sources = []\n",
    "      types = []\n",
    "      _ids = []\n",
    "      \n",
    "      curr_seq += 1\n",
    "\n",
    "  return events, times, times_max, np.array(embedded_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWY0Yv7ttQdL"
   },
   "outputs": [],
   "source": [
    "if(use_data_from_files):\n",
    "  all_events, all_events_ids, all_times, all_times_max, all_embedded_events = readData()\n",
    "else:\n",
    "  prep_time_start = timeit.default_timer()\n",
    "  all_events_ids, all_times, all_times_max, all_embedded_events = generateData(events_df, seq_len, windows_p, number_of_embedding_features)\n",
    "  all_events = encodeEvents(all_events_ids)\n",
    "  prep_time_stop = timeit.default_timer()\n",
    "  prep_time = prep_time_stop - prep_time_start\n",
    "\n",
    "  saveData(all_events, all_events_ids, all_times, all_times_max, all_embedded_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ervQPmdBxyEW",
    "outputId": "e19293be-9e28-4ebb-c898-20d48a68990d"
   },
   "outputs": [],
   "source": [
    "all_embedded_events.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embedded_events[all_embedded_events!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_features = []\n",
    "for seq in all_embedded_events:\n",
    "  event_features = []\n",
    "  for event in seq:\n",
    "    if model == 'W2V' or model == 'TW2V':\n",
    "      features =[event[0],event[1]] * int( number_of_spatio_features /2)\n",
    "    else:\n",
    "      features = [event[0]] * number_of_spatio_features\n",
    "    event_features.append(features)\n",
    "  seq_features.append(event_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_features = np.array(seq_features)\n",
    "print(seq_features.shape)\n",
    "print(seq_features[0][1])\n",
    "print(all_embedded_events[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embedded_events = seq_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZucKFgmtitr"
   },
   "outputs": [],
   "source": [
    "test_train_split = 0.8\n",
    "num_of_seqs = all_events.shape[0]\n",
    "num_of_train_seqs = math.ceil(num_of_seqs * test_train_split)\n",
    "split_details = [num_of_train_seqs]\n",
    "\n",
    "train_events, test_events = np.split(all_events, split_details)\n",
    "train_events_ids, test_events_ids = np.split(all_events_ids, split_details)\n",
    "train_times, test_times = np.split(all_times, split_details)\n",
    "train_times_max, test_times_max = np.split(all_times_max, split_details)\n",
    "train_embedding_features, test_embedding_features = np.split(all_embedded_events, split_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xc5hcGcetnz5",
    "outputId": "8ed38415-eecc-4f1d-9cca-91477cd34b2f"
   },
   "outputs": [],
   "source": [
    "print(all_events.shape)\n",
    "print(all_events_ids.shape)\n",
    "print(all_times.shape)\n",
    "print(all_times_max.shape)\n",
    "print(all_embedded_events.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcYVH-TxZmz2",
    "outputId": "aeec45c2-3825-4a60-d824-d74a6d9b0570"
   },
   "outputs": [],
   "source": [
    "cuda = getCuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUcFWo9et5kh"
   },
   "outputs": [],
   "source": [
    "t_events, t_events_ids, t_times, t_times_max, t_embedded_events = toTensor(train_events, train_events_ids, train_times, train_times_max, train_embedding_features, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fL702Ydff9Xw"
   },
   "outputs": [],
   "source": [
    "class W2VNNLSTM(nn.Module):\n",
    "  # input matrix size:  (batch_size, sequence_length, event_features_length)\n",
    "  # weight matrix size:  (event_features_length, output_size)\n",
    "  # output_size = hidden_size\n",
    "  # output_size:  (batch_size, output_size) for each of element on the sequence\n",
    "  # output_size:  (batch_size, sequence_length, output_size) for all elements on the sequence\n",
    "  # weight_matrix: (output_size, output_size) \n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, number_of_spatio_features: int, transform_fun, sigma = torch.sigmoid):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.number_of_spatio_features = number_of_spatio_features\n",
    "        self.transform_fun = transform_fun\n",
    "\n",
    "        self.W = nn.Linear(self.input_size, 7*self.hidden_size)\n",
    "        # 1 - input gate \n",
    "        # 2 - forget gate\n",
    "        # 3 - z gate\n",
    "        # 4 - output gate\n",
    "        # 5 - i_dash gate\n",
    "        # 6 - f_dash gate\n",
    "        # 7 - delta gate\n",
    "        self.U = nn.Linear(self.hidden_size, 7*self.hidden_size)\n",
    "        # embedding features\n",
    "        self.E = nn.Linear(self.number_of_spatio_features, 7*self.hidden_size)\n",
    "        # output mapping from hidden vectors to unnormalized intensity\n",
    "        self.L = nn.Linear(self.hidden_size, self.input_size, bias=False)\n",
    "        \n",
    "        self.initWeights()\n",
    "        self.sigma = sigma\n",
    "        self.scale = nn.Parameter(torch.ones(self.input_size, requires_grad=True))\n",
    "\n",
    "    def initWeights(self):\n",
    "        nn.init.normal_(self.W.weight, mean=0.0, std=0.01)\n",
    "        nn.init.normal_(self.W.bias, mean=0.0, std=0.01)\n",
    "        nn.init.normal_(self.E.weight, mean=0.0, std=0.01)\n",
    "        nn.init.normal_(self.E.bias, mean=0.0, std=0.01)\n",
    "        nn.init.normal_(self.U.weight, mean=0.0, std=0.01)\n",
    "        nn.init.normal_(self.U.bias, mean=0.0, std=0.01)\n",
    "        nn.init.normal_(self.L.weight, mean=0.0, std=0.01)\n",
    "\n",
    "    def calcLambdaK(self, h_t): \n",
    "      if(self.transform_fun == 'RELU'):\n",
    "        return torch.relu(h_t)\n",
    "      if self.transform_fun == 'SOFTPLUS':\n",
    "        return torch.log(1 + torch.exp(self.L(h_t)))\n",
    "      if self.transform_fun == 'SOFTPLUS_SCALE':\n",
    "        return self.scale * torch.log(1 + torch.exp(self.L(h_t)/self.scale))\n",
    "      raise Exception('Unsupported transform_fun')\n",
    "   \n",
    "    def forward(self, events, times, embedded_events):\n",
    "        batch_size, batch_length, _ = events.shape\n",
    "        delta_seq = torch.zeros((batch_size, batch_length, self.hidden_size), device=cuda)\n",
    "        o_seq = torch.zeros((batch_size, batch_length, self.hidden_size), device=cuda)\n",
    "        c_seq = torch.zeros((batch_size, batch_length, self.hidden_size), device=cuda)\n",
    "        c_dash_seq = torch.zeros((batch_size, batch_length, self.hidden_size), device=cuda)\n",
    "\n",
    "        lambda_seq = torch.zeros((batch_size, batch_length, self.input_size), device=cuda)\n",
    "        h_t = torch.zeros((batch_size, self.hidden_size), device=cuda).float()\n",
    "        c_t = torch.zeros((batch_size, self.hidden_size), device=cuda).float()\n",
    "        c_dash = torch.zeros((batch_size, self.hidden_size), device=cuda).float()\n",
    "\n",
    "        for event_idx in range(batch_length):\n",
    "          x = events[:, event_idx, :]\n",
    "          x_embed = embedded_events[:, event_idx, :]\n",
    "\n",
    "          outs = self.W(x) + self.U(h_t) + self.E(x_embed)\n",
    "\n",
    "          # 1 - input gate \n",
    "          # 2 - forget gate\n",
    "          # 3 - z gate\n",
    "          # 4 - output gate\n",
    "          # 5 - i_dash gate\n",
    "          # 6 - f_dash gate\n",
    "          # 7 - delta gate\n",
    "          i, f, z, o, i_dash, f_dash, delta = (\n",
    "                self.sigma(outs[:, :self.hidden_size]),\n",
    "                self.sigma(outs[:, self.hidden_size:self.hidden_size*2]), \n",
    "                2 * self.sigma(outs[:, self.hidden_size*2:self.hidden_size*3]) - 1, \n",
    "                self.sigma(outs[:, self.hidden_size*3:self.hidden_size*4]),\n",
    "                self.sigma(outs[:, self.hidden_size*4:self.hidden_size*5]), \n",
    "                self.sigma(outs[:, self.hidden_size*5:self.hidden_size*6]), \n",
    "                F.softplus(outs[:, self.hidden_size*6:self.hidden_size*7]), \n",
    "          )\n",
    "\n",
    "          c = f * c_t + i * z\n",
    "          c_dash = f_dash * c_dash + i_dash * z\n",
    "          t_now = times[:, event_idx].view(-1, 1)\n",
    "          t_next = times[:, event_idx + 1].view(-1, 1) \n",
    "          c_t = c_dash + (c - c_dash) * torch.exp(-delta * (t_next - t_now))\n",
    "          h_t = o * (2 * self.sigma(2 * c_t) - 1)\n",
    "          lambda_k = self.calcLambdaK(h_t)\n",
    "\n",
    "          c_seq[:, event_idx, :] = c\n",
    "          c_dash_seq[:, event_idx, :] = c_dash\n",
    "          o_seq[:, event_idx, :] = o\n",
    "          delta_seq[:, event_idx, :] = delta\n",
    "          lambda_seq[:, event_idx, :] = lambda_k\n",
    "\n",
    "        return c_seq, c_dash_seq, o_seq, delta_seq, lambda_seq\n",
    "\n",
    "    def getLoss(self, events_ids, times, max_times, c_seq, c_dash_seq, o_seq, delta_seq, lambda_seq):\n",
    "        batch_size, batch_length = events_ids.shape\n",
    "        original_loss = 0.\n",
    "\n",
    "        for ev in range(batch_length):\n",
    "            lambdas = lambda_seq[torch.arange(batch_size), ev, events_ids[:, ev]]\n",
    "            log_lambdas = torch.log(lambdas)\n",
    "            original_loss -= torch.sum(log_lambdas)\n",
    "\n",
    "        simulated_loss = 0.\n",
    "        trends = torch.rand((batch_size, batch_length), device=cuda) * max_times.view(-1, 1) # (1 x batch_size) to (batch_size x 1) to enable multiply\n",
    "        t_up = torch.searchsorted(times, trends)\n",
    "        I = torch.zeros((batch_size), device=cuda)\n",
    "        \n",
    "        for t_idx in range(batch_length):\n",
    "            T = trends[:, t_idx].view(-1,1)\n",
    "\n",
    "            idx = t_up[:, t_idx]\n",
    "            if torch.any(idx < 1):\n",
    "                continue\n",
    "            \n",
    "            t = times.gather(1, (idx-1).view(-1, 1))\n",
    "            \n",
    "            c_seq_x_dim = c_seq.shape[0]\n",
    "            c = c_seq[torch.arange(c_seq_x_dim), idx-1]\n",
    "            c_dash = c_dash_seq[torch.arange(c_seq_x_dim), idx-1]\n",
    "            delta = delta_seq[torch.arange(c_seq_x_dim), idx-1]\n",
    "            o = o_seq[torch.arange(c_seq_x_dim), idx-1]\n",
    "            c_t = c_dash + (c - c_dash)*torch.exp(-delta * (T - t))\n",
    "            h_t = o * (2 * self.sigma(2 * c_t) - 1)\n",
    "            lambda_k = self.calcLambdaK(h_t)\n",
    "            lambda_total = torch.sum(lambda_k, dim=1)\n",
    "            I += lambda_total * max_times / batch_length\n",
    "        \n",
    "        simulated_loss = torch.sum(I, dim=0)\n",
    "        loss = original_loss + simulated_loss\n",
    "\n",
    "        return loss / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = getPathDir()\n",
    "\n",
    "reload = False \n",
    "if reload: \n",
    "    last_epoch = 15\n",
    "    try:\n",
    "        net = torch.load(\"./social-interactions/{}/model_{}_{}_{}_{}_{}__{}.pt\".format(dir, seq_len, database, batch_size, hidden_size, windows_p, last_epoch))\n",
    "    except:\n",
    "        print(\"No saved network found. Starting from scratch\")\n",
    "        net = W2VNNLSTM(n_types, hidden_size, number_of_spatio_features, transform_fun)\n",
    "else: \n",
    "    net = W2VNNLSTM(n_types, hidden_size, number_of_spatio_features, transform_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rG2nqtH1rll6",
    "outputId": "8a4bd175-bef5-42ac-e386-2ff49f3cb2f8"
   },
   "outputs": [],
   "source": [
    "net.to(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5zK4c_NomvX",
    "outputId": "68ef8408-389e-4afb-a578-090f27964a50"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "losses = []\n",
    "train_time_start = timeit.default_timer()\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "\n",
    "    print(\"\\nEpoch:{}\".format(epoch+1), flush=True)\n",
    "    batch_loss = []  \n",
    "    n_train = t_events.shape[0]\n",
    "\n",
    "    perm = torch.randperm(n_train)\n",
    "    for batch_index in tqdm(range(0, n_train, batch_size), position=0, leave=True):\n",
    "        batch_begin = batch_index \n",
    "        batch_end = batch_index + batch_size\n",
    "        batch_events = t_events[perm][batch_begin:batch_end]\n",
    "        batch_events_ids = t_events_ids[perm][batch_begin:batch_end]\n",
    "        batch_times = t_times[perm][batch_begin:batch_end]\n",
    "        batch_max_times = t_times_max[perm][batch_begin:batch_end]\n",
    "        batch_embedded_events = t_embedded_events[perm][batch_begin:batch_end]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        c_seq, c_dash_seq, o_seq, delta_seq, lambda_seq = net.forward(batch_events, batch_times, batch_embedded_events)\n",
    "        loss = net.getLoss(batch_events_ids, batch_times, batch_max_times, c_seq, c_dash_seq, o_seq, delta_seq, lambda_seq)\n",
    "        batch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_batch_loss = mean(batch_loss)\n",
    "    print(mean_batch_loss)\n",
    "    if(math.isnan(mean_batch_loss) or not math.isfinite(mean_batch_loss)):\n",
    "      break\n",
    "    losses.append(mean_batch_loss)\n",
    "    torch.save(net, \"./social-interactions/{}/model_{}_{}_{}_{}_{}__{}.pt\".format(dir, seq_len, database, batch_size, hidden_size, windows_p, epoch+1))\n",
    "\n",
    "train_time_stop = timeit.default_timer()\n",
    "train_time = train_time_stop - train_time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjscZY0V1OSG"
   },
   "outputs": [],
   "source": [
    "print('Długość sekwencji: {}'.format(seq_len))\n",
    "print('Liczba rodzajów interakcji: {}'.format(n_types))\n",
    "print('Liczba epok: {}'.format(n_epoch))\n",
    "print('Rozmair porcji (batch): {}'.format(batch_size))\n",
    "print('Liczba ukrytych neuronów sieci: {}'.format(hidden_size))\n",
    "print('Czas trenowania: {}'.format(train_time))\n",
    "print('Liczba podziałów temporalnych: {}'.format(windows_p))\n",
    "print('Czas przygotowania sekwencji: {}'.format(prep_time))\n",
    "print('Sztuczny zbiór danych: {}'.format(useSyntheticData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsIyYZ2zDF9f"
   },
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01yxTlscsYAu"
   },
   "outputs": [],
   "source": [
    "def singleSeqThinningAlgorithm(net, seq_events, seq_events_ids, seq_times, embedded_events):\n",
    "  n_events, n_types = seq_events.shape\n",
    "  c_seq, c_dash_seq, o_seq, delta_seq, lambda_seq = net.forward(seq_events.view(-1, n_events, n_types), seq_times.view(-1, n_events+1), embedded_events.view(-1, n_events, number_of_spatio_features))\n",
    "\n",
    "  events_predicted = torch.zeros(n_events)\n",
    "  times_predicted = torch.zeros(n_events)\n",
    "  correct_pred = torch.zeros(n_events)\n",
    "\n",
    "  for i in range(n_events):\n",
    "    c = c_seq[:, i]\n",
    "    c_dash = c_dash_seq[:, i]\n",
    "    o = o_seq[:, i]\n",
    "    delta = delta_seq[:, i]\n",
    "    t = seq_times[i].item()\n",
    "\n",
    "    c_max = torch.max(c, c_dash)\n",
    "    h_max = o * (2 * net.sigma(2 * c_max) - 1)\n",
    "    lambda_max = net.scale * torch.log(1 + torch.exp(net.L(h_max)/net.scale)).view(n_types)\n",
    "    lambda_max_total = torch.sum(lambda_max).item()\n",
    "\n",
    "    temp_t = t\n",
    "    lambda_total = math.inf\n",
    "\n",
    "    stop = False\n",
    "    stop_arr = []\n",
    "    while (not stop):\n",
    "      delta_time = random.expovariate(lambda_max_total)\n",
    "      temp_t += delta_time\n",
    "        \n",
    "      c_t = c_dash + (c - c_dash) * torch.exp(-delta * (temp_t - t))\n",
    "      h_t = o * (2 * net.sigma(2 * c_t) - 1)\n",
    "      lambda_k = net.scale * torch.log(1 + torch.exp(net.L(h_t)/net.scale)).view(n_types)\n",
    "\n",
    "      u = np.random.rand()\n",
    "      stop_arr = (u * lambda_max > lambda_k).nonzero().squeeze(1)\n",
    "      stop = len(stop_arr) > 0\n",
    "\n",
    "    for _, ev in enumerate(torch.argsort(lambda_k, descending=True)):\n",
    "      ev = ev.item()\n",
    "      if(ev in stop_arr):\n",
    "        times_predicted[i] = temp_t\n",
    "        events_predicted[i] = ev\n",
    "        if ev == seq_events_ids[i].item():\n",
    "          correct_pred[i] = 1\n",
    "        break;\n",
    "\n",
    "  return times_predicted, events_predicted, correct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIYC7PqRtKhP"
   },
   "outputs": [],
   "source": [
    "def getDetails(t_events, t_events_ids, t_times, seq_idx, seq_len, t_embedded_events_test):\n",
    "  return t_events[seq_idx, :seq_len], t_events_ids[seq_idx, :seq_len], t_times[seq_idx, :seq_len+1], t_embedded_events_test[seq_idx, :seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "797xxyCEqEzy"
   },
   "outputs": [],
   "source": [
    "def getAccByType(pred_events, true_events):\n",
    "  number_of_types = np.zeros(n_types)\n",
    "  number_of_true_pred_types = np.zeros(n_types)\n",
    "\n",
    "  for index, true_event in enumerate(true_events):\n",
    "    number_of_types[true_event] += 1\n",
    "    \n",
    "    if(true_event == pred_events[index]):\n",
    "      number_of_true_pred_types[true_event] += 1\n",
    "\n",
    "  for index, number_of_type in enumerate(number_of_types):\n",
    "    if(number_of_type == 0):\n",
    "      number_of_true_pred_types[index] = 1\n",
    "      number_of_types[index] = -1\n",
    "\n",
    "  acc_by_type = number_of_true_pred_types / number_of_types\n",
    "\n",
    "  return number_of_true_pred_types, number_of_types, acc_by_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVTzZMbSulGh"
   },
   "outputs": [],
   "source": [
    "def plotAccByType(acc_by_type):\n",
    "  seq_size = acc_by_type.shape[0]\n",
    "\n",
    "  acc_by_type = np.where(acc_by_type == -1, np.nan, acc_by_type)\n",
    "\n",
    "  plt.figure(figsize=(100,5))\n",
    "  plt.subplot(1,2,1)\n",
    "  for _type in range(n_types):\n",
    "      plt.plot(np.arange(0, seq_size, 1), acc_by_type[:,_type],'o', label=indexEventTypeSynMap(_type) if useSyntheticData else indexEventTypeMap(_type))\n",
    "\n",
    "\n",
    "  plt.legend(fontsize=14)\n",
    "  plt.ylabel(\"Dokładność predykcji\", fontsize=16)\n",
    "  plt.xlabel(\"Sekwencje\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsoyzEXrs9Gi"
   },
   "outputs": [],
   "source": [
    "n_test = test_events.shape[0]\n",
    "n_seq_max = test_events.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFlU0JkWZo6k"
   },
   "outputs": [],
   "source": [
    "cuda = getCuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Y65NVWCs-0n"
   },
   "outputs": [],
   "source": [
    "t_events_test, t_events_ids_test, t_times_test, t_times_max_test, t_embedded_events_test = toTensor(test_events, test_events_ids, test_times, test_times_max, test_embedding_features, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmkzhWaVqfmp"
   },
   "outputs": [],
   "source": [
    "def calcAccForAllSequences():\n",
    "    all_pred_times = torch.ones(n_test, n_seq_max) * -1 \n",
    "    acc_by_type = np.zeros((n_test,  n_types)) * -1\n",
    "    acc_arr = torch.zeros(n_test)\n",
    "\n",
    "    for idx in tqdm(range(n_test), position=0, leave=True):\n",
    "        seq_len = len(test_events[idx])\n",
    "        seq_events, seq_labels, times, embedded_events = getDetails(t_events, t_events_ids, t_times, idx, seq_len, t_embedded_events_test)\n",
    "        times_predicted, events_predicted, correct_pred  = singleSeqThinningAlgorithm(net, seq_events, seq_labels, times, embedded_events)\n",
    "        number_of_true_pred_types, number_of_types, acc_values_by_type = getAccByType(events_predicted, seq_labels)\n",
    "        acc_by_type[idx] = acc_values_by_type\n",
    "        acc_arr[idx] = torch.sum(correct_pred)/len(correct_pred)\n",
    "        all_pred_times[idx, :seq_len] = times_predicted\n",
    "    \n",
    "    return acc_arr, all_pred_times, events_predicted, acc_by_type\n",
    "    \n",
    "acc_arr, all_pred_times, events_predicted, acc_by_type = calcAccForAllSequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qQn17yWw45x"
   },
   "outputs": [],
   "source": [
    "plotAccByType(acc_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUjisu0GqhZ6"
   },
   "outputs": [],
   "source": [
    "acc_arr = acc_arr.numpy()\n",
    "all_pred_times = all_pred_times.numpy()\n",
    "events_predicted = events_predicted.numpy()\n",
    "\n",
    "print(\"Średnia dokładność predykcji przyszłych rodzajów zdarzeń: {:.4f}%\".format(np.mean(acc_arr)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMxMjOAwqiC0"
   },
   "outputs": [],
   "source": [
    "seq_avgs = np.zeros(n_test)\n",
    "seq_log_avgs = np.zeros(n_test)\n",
    "\n",
    "for seq in range(n_test):\n",
    "    seq_len = len(test_events[seq])\n",
    "    actual_time = test_times[seq, 1:seq_len]\n",
    "    actual_time = [i if i != 0 else 1 for i in actual_time]\n",
    "    predicted_time = all_pred_times[seq, 1:seq_len]\n",
    "    dT2 = (predicted_time - actual_time)**2\n",
    "    dT2_avg = np.sum(dT2)/seq_len\n",
    "    seq_avgs[seq] = dT2_avg\n",
    "    \n",
    "print(\"Pierwiastek z uśrednionego błędu średniokwadratowego wartości czasu dla {} sekwencji testowych: {}\".format(n_test, np.sqrt(np.mean(seq_avgs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAccForAllEpoches():\n",
    "    for epoch in range(n_epoch):\n",
    "        try:\n",
    "            net = torch.load(\"./social-interactions/{}/model_{}_{}_{}_{}_{}__{}.pt\".format(dir, seq_len, database, batch_size, hidden_size, windows_p, epoch+1))\n",
    "\n",
    "        except:\n",
    "            print(\"No saved network found. Exit\")\n",
    "            return \n",
    "\n",
    "        net.to(cuda)\n",
    "\n",
    "        all_pred_times = torch.ones(n_test, n_seq_max)*-1  # negative time means no event occurred\n",
    "        acc_by_type = np.zeros((n_test,  n_types)) * -1\n",
    "        acc_arr = torch.zeros(n_test)\n",
    "\n",
    "        for idx in tqdm(range(n_test), position=0, leave=True):\n",
    "          seq_len = len(test_events[idx])\n",
    "          seq_events, seq_labels, times, embedded_events = getDetails(t_events, t_events_ids, t_times, idx, seq_len, t_embedded_events_test)\n",
    "          times_predicted, events_predicted, correct_pred  = singleSeqThinningAlgorithm(net, seq_events, seq_labels, times, embedded_events)\n",
    "          number_of_true_pred_types, number_of_types, acc_values_by_type = getAccByType(events_predicted, seq_labels)\n",
    "\n",
    "          acc_by_type[idx] = acc_values_by_type\n",
    "          acc_arr[idx] = torch.sum(correct_pred)/len(correct_pred)\n",
    "          all_pred_times[idx, :seq_len] = times_predicted\n",
    "        acc_arr = acc_arr.numpy()\n",
    "        all_pred_times = all_pred_times.numpy()\n",
    "        events_predicted = events_predicted.numpy()\n",
    "\n",
    "        print(\"Średnia dokładność predykcji przyszłych rodzajów zdarzeń dla {}: {:.4f}%\".format(epoch+1, np.mean(acc_arr)*100))\n",
    "        \n",
    "calcAccForAllEpoches()        "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NN weg2vec nethealth.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
